{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "device = 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "print(device)\n",
    "block_size = 8\n",
    "batch_size = 4\n",
    "max_iters = 1000\n",
    "learning_rate = 3e-4\n",
    "eval_iters = 250\n",
    "#dropouts = 0.2 #drop neurons so we dont overfit, help model train better "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '\"', '&', \"'\", '(', ')', '*', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "with open('wizard_of_oz.txt', 'r', encoding = 'utf-8') as f:\n",
    "    text = f.read()\n",
    "    \n",
    "chars = sorted(set(text))\n",
    "print(chars)\n",
    "vocab_size = len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1,  1, 28, 39, 42, 39, 44, 32, 49,  1, 25, 38, 28,  1, 44, 32, 29,  1,\n",
      "        47, 33, 50, 25, 42, 28,  1, 33, 38,  1, 39, 50,  0,  0,  1,  1, 26, 49,\n",
      "         0,  0,  1,  1, 36, 11,  1, 30, 42, 25, 38, 35,  1, 26, 25, 45, 37,  0,\n",
      "         0,  1,  1, 25, 45, 44, 32, 39, 42,  1, 39, 30,  1, 44, 32, 29,  1, 47,\n",
      "        33, 50, 25, 42, 28,  1, 39, 30,  1, 39, 50,  9,  1, 44, 32, 29,  1, 36,\n",
      "        25, 38, 28,  1, 39, 30,  1, 39, 50,  9])\n"
     ]
    }
   ],
   "source": [
    "string_to_int = { ch:i for i,ch in enumerate(chars) }\n",
    "int_to_string = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [string_to_int[c] for c in s]\n",
    "decode = lambda l: ''.join([int_to_string[i] for i in l])\n",
    "\n",
    "data = torch.tensor(encode(text), dtype = torch.long)\n",
    "print(data[:100]) #Here we are encoding the text. We have output the first 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs: \n",
      "tensor([[58,  0, 69, 71, 58, 73, 73, 78],\n",
      "        [ 1, 68, 59,  1, 74, 72,  1, 73],\n",
      "        [ 1, 58, 75, 58, 71,  1, 65, 58],\n",
      "        [76, 61, 62, 56, 61,  1, 57, 62]], device='mps:0')\n",
      "target: \n",
      "tensor([[ 0, 69, 71, 58, 73, 73, 78,  1],\n",
      "        [68, 59,  1, 74, 72,  1, 73, 61],\n",
      "        [58, 75, 58, 71,  1, 65, 58, 54],\n",
      "        [61, 62, 56, 61,  1, 57, 62, 57]], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "n = int(0.8*len(data))\n",
    "train_data = data[:n] #80% train\n",
    "val_data = data[n:] #20% test\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,)) #we are getting starting indices for the batches\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix]) #creating input\n",
    "    y = torch.stack([data[i+1:i+block_size + 1] for i in ix]) #creating target, shifting by 1\n",
    "    x, y = x.to(device), y.to(device) #we are moving the tensors to MPS location\n",
    "    return x,y\n",
    "\n",
    "x,y = get_batch('train')\n",
    "print('inputs: ')\n",
    "print(x)\n",
    "print('target: ')\n",
    "print(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad() #not using gradients to avoid additional computations and we are just doing evaluations. No need to update weights\n",
    "def estimate_loss(): #creating a function to evaluate the model's loss on training and evaluations\n",
    "    out = {}\n",
    "    model.eval() #putting model in evaluation mode\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y) #the model here is the bigram model. Doing a forward pass through the model.\n",
    "            losses[k] = loss.item() #storing the loss value in our losses tensor\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out #sample output: {'train': 0.15, 'val': 0.22}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ":S-sm\"r76p;Hw6XlGO39]5kt\n",
      "5rTBuaD4 ]&GYhO\n",
      "JMW1)WwCrKDQmbqrY*D(7tH[Lq7iG8n83f76Ns8YV\"HZfmj4WdD&FQhp;I5)7dds(nC!q[*Zs 9SsM0.z*81?stT;3)4OdRhNZT]1e;6&EpJ_Uif27st5*J\"4Wk?.HzxhRF:m::&FhEP&29wnlISz3ALq[)xyU S*Du8Mj(ziP0fheT_;3AUnlyKaRdRh9KQb[qr?('9I[LCZdahbgOggvaJ\";V*k8L8kM8V9pCEYq[X?Xl4[B S lahYwHVN!3G\"\n",
      "CseR4LK&iBNS:l9I59)Yb6TBx'E'E-rJLE7Cen GSbEs Mlnqr03-'Y;.cBP-[,UEHD)mBVZDFM;:hM\n",
      "a\"Rw9eL!*oT\n",
      "OzrJR6*DKqRhbu :hODKSh]h]T*ngDDwnoxgZbW9IS7Sze!dlnWpv4YgkfK,TNHd1rn8V&hB[2cJYb]AN&FSs(YRw?x8i1c_;.Ey!2(WyKy?s\n"
     ]
    }
   ],
   "source": [
    "# Very simple neural network with only one layer, embedding layer\n",

    "class BigramLanguageModel(nn.Module): #using base model for PyTorch neural network model\n",
    "    #EMBEDDING LAYER\n",
    "    def __init__(self, vocab_size): #initialize the model, vocab_size is the number of unique tokens\n",
    "        super().__init__() #access methods and properties on the parent class nn.Module. It ensures that all internal mechanisms of the PyTorch model are initialized, such as Tracking model parameters, Enabling device management, Setting up hooks for backpropagation.\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size) #allows for simplicity in the later layers\n",
    "    #FORWARD PASS    \n",
    "    def forward(self, index, targets = None):\n",
    "        logits = self.token_embedding_table(index) #creates embedding layer\n",
    "        #initializes an embedding matrix of size [vocab_size, vocab_size] with random values.\n",
    "        #during training, the embedding vectors and weights are updated using backpropagation based on the loss\n",
    "        #As the model trains, the embedding vectors evolve to capture semantic relationships, making logits more accurate\n",
    "\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape  #B batch size, T for Token Size, C for Vocab Size\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets) #compares the model's guess (logits) with the correct one (targets)\n",
    "            \n",
    "        return logits, loss\n",
    "\n",
    "   
    "\n",
    "    def generate(self, index, max_new_tokens): #here we are generating text\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, loss = self.forward(index)\n",
    "            logits = logits[:, -1, :] #extracting logits for the last token\n",
    "            probs = F.softmax(logits, dim = -1) #Using softmax to get probabilities\n",
    "            index_next = torch.multinomial(probs, num_samples = 1) #picking the highest probabaility\n",
    "            index = torch.cat((index, index_next), dim = 1) #Concatenating the character\n",
    "        return index\n",
    "\n",
    "\n",
    "model = BigramLanguageModel(vocab_size)\n",
    "m = model.to(device) #move to mps\n",
    "\n",
    "# context = torch.zeros((1, 1), dtype = torch.long, device = device) # we are starting at index 0\n",
    "# generated_chars = decode(m.generate(context, max_new_tokens = 500)[0].tolist()) # converts the list of token indices (generated_indices) back into human-readable characters or text\n",
    "# print(generated_chars)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, train loss 4.601349830627441, val loss: 4.599634170532227\n",
      "step 250, train loss 4.551927089691162, val loss: 4.538822174072266\n",
      "step 500, train loss 4.478064060211182, val loss: 4.481635570526123\n",
      "step 750, train loss 4.431306838989258, val loss: 4.417907238006592\n",
      "4.558805465698242\n"
     ]
    }
   ],
   "source": [
    "#pytorch optimizer\n",
    "#TRAINING OCCURS HERE\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = learning_rate) #This will help to adjust the model's parameters using gradients\n",
    "#learning rate controls how big and small the adjustments are\n",
    "for iter in range(max_iters): #max_iters is our training steps\n",
    "    if iter % eval_iters == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}, train loss {losses['train']}, val loss: {losses['val']}\")\n",
    "\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    logits, loss = model.forward(xb, yb) #xb is our input tokens yb represents the target tokens\n",
    "    optimizer.zero_grad(set_to_none = True) #zero out the previous gradient\n",
    "    loss.backward() # computes the gradients of the loss with respect to the model parameters using backpropagation\n",
    "    optimizer.step() #Adjust the parameters using the gradients so that the next forward pass will make better predictions\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the model is trained first (adjusting weights via backpropagation) and then used for generation (predicting tokens with the learned parameters).\n",
    "context = torch.zeros((1, 1), dtype = torch.long, device = device)\n",
    "generated_chars = decode(m.generate(context, max_new_tokens = 500)[0].tolist())\n",
    "print(generated_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# block_size = 8\n",
    "\n",
    "# x = train_data[:block_size]\n",
    "# y = train_data[1:block_size+1]\n",
    "\n",
    "# for t in range(block_size):\n",
    "#     context  = x[:t+1]\n",
    "#     target = y[t]\n",
    "#     print('when input is', context, 'target is ', target)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu kernel",
   "language": "python",
   "name": "gpu_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
